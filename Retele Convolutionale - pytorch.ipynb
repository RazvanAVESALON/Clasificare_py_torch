{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Rețele Convoluționale - Convolutional Neural Networks (CNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"1.-Introducere\">1. Introducere<a class=\"anchor-link\" href=\"#1.-Introducere\">¶</a></h2><p>In lucrarea trecuta s-a studiat problema clasificării unei baze de date simple (MNIST) cu ajutorul unei rețele de tip perceptron multistrat (Multilayer Perceptron - MLP). Pe parcursul acestui studiu de caz s-au abordat pașii elementari pentru rezolvarea unei astfel de probleme (alegerea arhitecturii, a funcției <i>loss</i>, a optimizatorului, a ratei de învățare, împărțirea bazei de date in <i>batch-uri</i> precum si bucla de învățare.</p>\n",
    "\n",
    "<p>In continuare, soluția simplă din laboratorul trecut va fi extinsă prin folosirea arhitecturilor mult mai puternice de tip rețele convoluționale (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"2.-Motivație-și-aspecte-generale\">2. Motivație și aspecte generale<a class=\"anchor-link\" href=\"#2.-Motivatie-si-aspecte-generale\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Rețeaua neuronală folosită anterior a utilizat drept trăsături de intrare chiar pixelii constituenți ai imaginilor. Deși rezultatele au fost satisfăcătoare, acest deznodământ nu ar fi fost la fel de probabil pentru baze de date mai complicate. \n",
    "In general, valorile directe ale pixelilor nu sunt considerate a fi trăsături puternice. Cu acest scop se utilizează extractoare de trăsături, precum Histogram of Oriented Gradients (HOG) si Local binary patterns (LBP), care surprind mai bine informatia spațială din fiecare zonă de interes sau din jurul fiecărui pixel.</p>\n",
    "\n",
    "<img src=\"image.png\"/><center>\n",
    "<img src=\"image2.png\"/><center>\n",
    "<img src=\"FeatureFace.jpg\"/><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning \n",
    "\n",
    "<p> Deep Learning este in momentul de fata cea mai des utilizata tehnica de machine learning deoarece permite invatarea unor reprezentari complexe prin intermediul retelelor neurale. Elementul principal al rețelelor neuronale este stratul. Stratul de neuroni este practic un modul de procesare a informatiei (puteti sa il vedeti ca un filtru aplicat pe datele de intrare). Astfel, datele intra intr-un strat, sunt procesate si transmise mai departe sub o alta forma. Mai exact, straturile extrag reprezentări din date, reprezentări care sunt mai semnificative pentru problema pe care vrem sa o rezolvam.\n",
    "</p>\n",
    "\n",
    "<img src=\"features.png\"/>\n",
    "\n",
    " In computer vision filtrarea se realizeaza prin intermediul operatiei de convolutie.\n",
    " \n",
    "<img src=\"filter.png\"/>\n",
    "\n",
    "<img src=\"filtering.png\"/>\n",
    "\n",
    "Ideea de convolutie a fost astfel adaptata in reteaua neurala prin intermediul stratului convolutional.\n",
    "\n",
    "## Rețele convoluționale\n",
    "<p>Rețelele convoluționale au adus o serie de îmbunătățiri in ceea ce privește algoritmii de <i>machine learning</i>. O parte dintre acestea vor fi discutate ulterior. Drept punct de plecare se va face referire la schema generala a unei rețele convoluționale:\n",
    "<img src=\"cnn.jpeg\"/></p><center>Schema generala a unei rețele convoluționale</center>\n",
    "<p>Se pot observa doua zone principale:</p>\n",
    "<ul>\n",
    "<li>Prima, formata din straturile de tip Convolution si Pooling</li>\n",
    "<li>A doua, formata din straturi <i>Fully Connected (Linear)</i> - straturi de neuroni clasici</li>\n",
    "</ul>\n",
    "<p>Prima zona s-a dovedit a avea rolul de extractor de trăsături. Primele straturi convoluționale extrag informații de tip contururi, iar acestea devin mai complexe odată cu parcurgerea rețelei. S-a observat ca trăsăturile ieșite după prima zona a rețelei sunt puternice, in general fiind mai reprezentative decât trăsăturile obținute prin aplicarea algoritmilor HoG sau LBP.</p>\n",
    "<p>A doua zona, cu straturile <i>Fully Connected</i> este practic o rețea MLP aplicata trăsăturilor extrase de zona convoluțională a rețelei.</p>\n",
    "\n",
    "<h3 id=\"2.1.-Straturile-convolutionale\">2.1. Straturile <i>convoluționale</i><a class=\"anchor-link\" href=\"#2.1.-Straturile-convolutionale\">¶</a></h3><p>Acest tip de strat este unitatea de baza din noile arhitecturi. Făcând o analogie cu procedeele consacrate din prelucrarea imaginilor, un strat convoluțional realizează o serie de operații de filtrare liniara pe matricea de la intrarea in strat. Aceasta matrice poate fi imaginea in sine, sau rezultatul altui strat, denumit <i>feature map</i> (harta de trăsături). Un fapt relevant in acest caz este ca filtrul de convoluție are adâncimea matricei de la intrare (ex. 3 pentru o imagine RGB).\n",
    "    \n",
    "<img src=\"conv.png\"/>  \n",
    "    \n",
    "De la modul in sine in care funcționează stratul convoluțional se pot observa diferențe fata de modul de funcționare al MLP, precum si proprietăți relevante:</p>\n",
    "<ul>\n",
    "<li><i>sparse interactions</i> (interactiuni \"rare\"): spre deosebire de MLP, unde fiecare neuron din stratul $N$ interacționa cu toți neuronii din stratul $N+1$, la un CNN doar o serie de neuroni din stratul curent vor participa la neuronul din stratul următor (zona denumita câmpul receptiv, <i>receptive field</i>). Acest fapt implica stocarea a mai putini parametri, si implicit operații mai puține;</li>\n",
    "<li>partajarea parametrilor: filtrul de convoluție este folosit pentru întreaga imagine, deci ponderile care conduc la un anume neuron din stratul următor sunt mereu aceleași. Aceasta trăsătura este pusa in contrast cu arhitectura MLP, unde fiecare pondere era folosita o singura data, pentru o pereche anume de neuroni;</li>\n",
    "<li>echivarianta la translatie a reprezentarilor: se refera la proprietatea ca daca imaginea de intrare suferă o transformare de tip translație, si harta de trăsături de ieșire va suferi aceeași modificare.</li>\n",
    "</ul>\n",
    "<p>Pentru a crea un strat convoluțional in Keras se folosește sintaxa:</p>\n",
    "\n",
    "<p><code>conv_x = torch.nn.Conv2d(in_channels, out_channels, kernel_size = [linii_filtru, coloane_filtru], stride=(pas_orizontal, pas_vertical), padding = 'same')(input)</code></p>\n",
    "<p>Forma corecta pentru <code>input</code> este de tipul <code>[nr_imag,linii,coloane,canale]</code>\n",
    "Argumentul <code>strides</code> se refera la pasul pe care îl face filtrul convoluțional după ce operează asupra zonei curente. Un pas de <code>(1,1)</code> înseamnă ca va parcurge toți neuronii.\n",
    "Argumentul <code>padding</code> se leagă de capetele imaginii. Se decide daca imaginea va fi bordata cu valori de 0, astfel încât sa fie parcurși toți pixelii din imagine (<code>'same'</code>) sau se vor ignora neuronii din capetele imaginii (<code>'valid'</code>). Daca <i>padding-ul</i> este <code>'same'</code> si pasul este <code>(1,1)</code>, atunci hârțile de trăsături vor avea același număr de linii si coloane ca stratul din care provin.</p>\n",
    "\n",
    "<p>Toate straturile convoluționale sunt urmate de o funcție de activare (in general Rectified Linear Unit - ReLU) pentru care există următoarea funcție\n",
    "<code> torch.nn.ReLU()(input) </code>.\n",
    "<img src=\"acti.jpg\"/>  \n",
    "\n",
    "<h3 id=\"2.2.-Straturile-de-pooling\">2.2. Straturile de <i>pooling</i><a class=\"anchor-link\" href=\"#2.2.-Straturile-de-pooling\">¶</a></h3><p>Operația de <i>pooling</i> este cel mai bine tradusa in limba romana ca o \"grupare\", o subeșantioane. Pe scurt, aceasta operație înlocuiește valoarea unei zone din imagine/harta de trăsături cu o statistica a acelei zone. Funcția de <i>max-pooling</i> este cea mai folosita in aplicații si înlocuiește valoarea dintr-o zona bine definita cu maximul acelei zone, rezultând într-o harta de trăsături mai mica, dar care păstrează cea mai relevanta statistica. In acest mod, pe lângă reducerea dimensionalității, se obține si o invarianta la translații mici.</p>\n",
    "<p>Modul de creare a unui strat de <i>max-pooling</i> in Keras care sa ia vecinatati 2x2:<br/> \n",
    "<code>pool_x = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2))(input)</code></p>\n",
    "<p>Reduce dimensiunea matricei de la intrare prin luarea valorii maxime peste fereastra definită de <code>'pool_size'</code> pentru fiecare dimensiune de-a lungul axei caracteristici. Fereastra este deplasată în fiecare dimensiune cu pasul specificat de <code>'strides'</code>.</p> \n",
    "\n",
    "<img src=\"pooling.png\"/>  \n",
    "\n",
    "<h3 id=\"2.3-Straturile-fully-connected\">2.3 Straturile <i>fully connected</i><a class=\"anchor-link\" href=\"#2.3-Straturile-fully-connected\">¶</a></h3><p>După cum a fost menționat anterior, aceste straturi sunt cele <b>dens conectate</b>, obișnuite dintr-un MLP. Deoarece hârțile de trăsături care rezultă din stratului convoluțional sau pooling sunt reprezentate ca matrici, înainte de a le putea folosi, <b>hârțile de trăsături trebuie aplatizate (vectorizate)</b>, adică matricea trebuie reprezentată ca un vector. Acest lucru se poate realizea cu funcția `view` sau cu clasa `Flatten` <br/>\n",
    "<code>flat = input.view(new_shape)</code>\n",
    "<br/>\n",
    "<code>flat = nn.Flatten()(input)</code>\n",
    "<br/>\n",
    "    \n",
    "Pentru a crea un strat dens:<br/>\n",
    "<code>fc = torch.nn.Linear(in_features = nr_neuroni_iesire_stratul_precedent, out_features = nr_neuroni_de_iesire)(input)</code><br/></p>\n",
    "\n",
    "<img src=\"feature_representation.png\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"3.-Arhitecturi-de-baza\">3. Arhitecturi de baza<a class=\"anchor-link\" href=\"#3.-Arhitecturi-de-baza\">¶</a></h2><p>Exista o multitudine de arhitecturi actuale, unele care au adus imbunatatiri marginale, altele care sunt specializate pe o anumita sarcina, dar cateva arhitecturi sunt considerate a fi pietre de temelie pentru domeniu. In continuare vor fi prezentate cateva arhitecturi care au atras o atentie foarte mare la momentul aparitiei lor.</p>\n",
    "\n",
    "<h3 id=\"3.1.-LeNet-5\">3.1. LeNet-5<a class=\"anchor-link\" href=\"#3.1.-LeNet-5\">¶</a></h3><p>Cea mai veche arhitectura convolutionala a fost prezentata in 1998 cu scopul de a recunoaste cifre scrise de mana in documente. A fost conceputa pentru imagini de rezolutie mica (32 x 32 pixeli) si, din cauza constrangerilor (la aceea vreme) cu privire la puterea de calcul, nu a prezentat o adancime mare (doar 2 straturi convolutionale cu filtre de 5 x 5 pixeli). Schema arhitecturii:\n",
    "<img src=\"lenet.png\"/></p><center>Arhitectura LeNet-5</center>\n",
    "\n",
    "<h3 id=\"3.2.-AlexNet\">3.2. AlexNet<a class=\"anchor-link\" href=\"#3.2.-AlexNet\">¶</a></h3><p>Dupa mai bine de un deceniu (in 2012), arhitectura AlexNet a fost prima arhitectura neuronala care a castigat concursul ImageNet Large Scale Visual Recognition Challenge (ILSVRC) cu o arhitectura avand 5 straturi convolutionale, imagini de intrare considerabil mai mari, filtre de convolutie mai mari in straturile initiale (11 x 11) cu pasi mai mari de parcurgere a imaginii si a folosit activari de tip <code>ReLU</code>. Aceasta arhitectura, mult mai puternica decat ce s-a vehiculat pana in acel moment , a fost antrenata cu ajutorul a doua GPU-uri performante. Schema arhitecturii:\n",
    "<img src=\"alexnet.png\"/></p><center>Arhitectura AlexNet</center>\n",
    "\n",
    "<h3 id=\"3.3-VGG\">3.3 VGG<a class=\"anchor-link\" href=\"#3.3-VGG\">¶</a></h3><p>Urmatorul pas important adus in domeniu a fost studiul impactului adancimii unei retele. In acest scop, retelele din familia VGG au demonstrat cresterea performantei odata cu adancimea. Reteaua VGG-19 (de la cele 19 straturi neuronale de orice tip) este printre cele mai mari retele utilizate in termeni de numar de parametri care trebuie invatati. In prezent, aceasta arhitectura este adesea folosita pentru trasaturile generale puternice extrase dupa ultimul strat convolutional, utile si in alte sarcini decat clasificarea. O alta observatie importanta este reprezentata de reducerea tuturor filtrelor de convolutie la dimensiunea 3 x 3 cu pas 1 la deplasare. Schema arhitecturii VGG-16:\n",
    "<img src=\"vgg16.png\"/></p><center>Arhitectura VGG-16</center>\n",
    "\n",
    "<h3 id=\"3.4.-ResNet\">3.4. ResNet<a class=\"anchor-link\" href=\"#3.4.-ResNet\">¶</a></h3><p>Aparuta mai recent ca celelalte arhitecturi prezentate, importanta acestei arhitecturi a fost uriasa, rezolvand problema <i>vanishing gradient</i>. Aceasta reprezenta scaderea puternica a gradientilor odata cu avansarea in retea in etapa de propagare inapoi. Practic, retelele cu un numar mare de straturi erau foarte greu de antrenat. Solutia acestui tip de arhitectura a fost introducerea blocului \"rezidual\", care presupunea ca la iesirea dintr-un bloc compus din mai multe convolutii se adauga si intrarea in bloc. Aceste conecxiuni de tip \"scurtatura\" (sau \"scurtcircuit\") au permis crearea unor retele mult mai adanci (inclusiv 1000 de straturi). Ca o consecinta a numarului crescut de straturi, s-a putut reduce numarul de filtre per strat, pastrand numarul de parametri care trebuiau invatati relativ redus. Arhitectura ResNet-34 este ilustrata alaturi de VGG-19 (care are un numar considerabil mai mare de parametri):\n",
    "<img src=\"resnet.png\"/></p><center>Arhitectura ResNet-34 in comparatie cu VGG-19 si o arhitectura cu 34 de straturi fara \"scurtaturi\"</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Baza de date MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyTorch version 1.10.1\n",
      "torchvision version 0.11.2\n",
      "CUDA available True\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "print(f\"pyTorch version {torch.__version__}\")\n",
    "print(f\"torchvision version {torchvision.__version__}\")\n",
    "print(f\"CUDA available {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26fa9dd3470>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "train_bs = 32\n",
    "test_bs = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr de imagini in setul de antrenare 121\n",
      "Nr de imagini in setul de test 14\n",
      "Dim primei imagini din Dataset tensor([[[0.1569, 0.0941, 0.0902,  ..., 0.0549, 0.0627, 0.0824],\n",
      "         [0.0784, 0.0157, 0.0157,  ..., 0.0353, 0.0157, 0.0314],\n",
      "         [0.0784, 0.0157, 0.0235,  ..., 0.1804, 0.0980, 0.0431],\n",
      "         ...,\n",
      "         [0.1569, 0.0902, 0.0745,  ..., 0.0980, 0.0431, 0.0824],\n",
      "         [0.1569, 0.0902, 0.0784,  ..., 0.0510, 0.0549, 0.0863],\n",
      "         [0.1922, 0.1333, 0.1176,  ..., 0.0824, 0.1059, 0.1333]],\n",
      "\n",
      "        [[0.1569, 0.0941, 0.0902,  ..., 0.0549, 0.0627, 0.0824],\n",
      "         [0.0784, 0.0157, 0.0157,  ..., 0.0353, 0.0157, 0.0314],\n",
      "         [0.0784, 0.0157, 0.0235,  ..., 0.1804, 0.0980, 0.0431],\n",
      "         ...,\n",
      "         [0.1569, 0.0902, 0.0745,  ..., 0.0980, 0.0431, 0.0824],\n",
      "         [0.1569, 0.0902, 0.0784,  ..., 0.0510, 0.0549, 0.0863],\n",
      "         [0.1922, 0.1333, 0.1176,  ..., 0.0824, 0.1059, 0.1333]],\n",
      "\n",
      "        [[0.1569, 0.0941, 0.0902,  ..., 0.0549, 0.0627, 0.0824],\n",
      "         [0.0784, 0.0157, 0.0157,  ..., 0.0353, 0.0157, 0.0314],\n",
      "         [0.0784, 0.0157, 0.0235,  ..., 0.1804, 0.0980, 0.0431],\n",
      "         ...,\n",
      "         [0.1569, 0.0902, 0.0745,  ..., 0.0980, 0.0431, 0.0824],\n",
      "         [0.1569, 0.0902, 0.0784,  ..., 0.0510, 0.0549, 0.0863],\n",
      "         [0.1922, 0.1333, 0.1176,  ..., 0.0824, 0.1059, 0.1333]]])\n",
      "Etichete pt prima imagine 0\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([ \n",
    "        T.Resize((64,64)),\n",
    "        \n",
    "        T.ToTensor(), # converts a PIL.Image or numpy array into torch.Tensor\n",
    "       \n",
    "        # T.Normalize((0.1307,), (0.3081,)), # Normalize the dataset with mean and std specified\n",
    "               ])\n",
    "data_dir=r\"D:\\ai intro\\AI intro\\5. Retele Neurale\\covid_dataset\"\n",
    "train_ds = dset.ImageFolder(data_dir+'/train',transform=transforms)\n",
    "test_ds = dset.ImageFolder(data_dir+'/test',transform=transforms)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=train_bs)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, shuffle=False, batch_size=test_bs)\n",
    "\n",
    "print(\"Nr de imagini in setul de antrenare\", len(train_ds))\n",
    "print(\"Nr de imagini in setul de test\", len(test_ds))\n",
    "\n",
    "print(\"Dim primei imagini din Dataset\", train_ds[0][0])\n",
    "print(\"Etichete pt prima imagine\", train_ds[0][1])\n",
    "\n",
    "n_classes = len(np.unique(train_ds.targets))\n",
    "print(np.unique(train_ds.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Arhitectura rețelei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Varianta 1: API secvențial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimension torch.Size([3, 64, 64])\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=valid)\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=valid)\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n1 = 16\n",
    "n2 = 32\n",
    "n3 = 64 \n",
    "image_dim = train_ds[0][0].shape\n",
    "print(\"Image dimension\", image_dim)\n",
    "\n",
    "# definim arhitectura retelei convoluționale\n",
    "network = nn.Sequential(\n",
    "    nn.Conv2d(3, n1, (5, 5), padding='valid'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Conv2d(n1, n2, (5, 5), padding='valid'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d((2, 2)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(n2*4*4, n3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n3, n_classes)\n",
    ")\n",
    "\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Varianta 2: Moștenire clasa `torch.nn.Module` \n",
    "Este o modalitate de a crea networke care sunt mai flexibile decât torch.nn.Sequential API. Prin moștenire se pot crea rețele cu topologie neliniară, straturi partajate și chiar intrări sau ieșiri multiple. Clasa-copil trebuie să implementeze funcțiile `__init__()` pentru a instanția straturile necesare și o funcție `forward()` unde se realizează calculele, input este trecut prin straturile rețelei și/sau alte funcții."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "CustomNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "  (linear1): Linear(in_features=8192, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (max_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n1 = 16\n",
    "n2 = 32\n",
    "n3 = 64\n",
    "\n",
    "class CustomNet(torch.nn.Module):\n",
    "    def __init__(self, input_nc, n1, n2, n3, n_classes):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_nc, n1, (5, 5), padding='same')\n",
    "        self.conv2 = nn.Conv2d(n1, n2, (5, 5), padding='same')\n",
    "        self.linear1 = nn.Linear(n2*16*16, 64) # 4*4 image dimension after 2 max_pooling if input 16x16 MNIST images\n",
    "        self.linear2 = nn.Linear(n3, n_classes)\n",
    "        self.max_pool = nn.MaxPool2d((2, 2))\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(x.shape)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        #print()\n",
    "        return x\n",
    "\n",
    "print (n_classes)\n",
    "\n",
    "network = CustomNet(3, n1, n2, n3, n_classes)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Antrenarea networkului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificarea functiei loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# definirea optimizatorului\n",
    "opt = torch.optim.Adam(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device  cuda:0\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 0: error 0.007164856651797891 accuracy 99.17355371900827\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 1: error 0.008723457518499345 accuracy 98.34710743801654\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 2: error 0.009362877492094412 accuracy 99.17355371900827\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 3: error 0.007411502709146589 accuracy 96.69421487603306\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 4: error 0.005897092603845522 accuracy 99.17355371900827\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 5: error 0.00469684021663852 accuracy 98.34710743801654\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([32, 16, 32, 32])\n",
      "torch.Size([32, 32, 16, 16])\n",
      "torch.Size([32, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "torch.Size([25, 16, 32, 32])\n",
      "torch.Size([25, 32, 16, 16])\n",
      "torch.Size([25, 8192])\n",
      "\n",
      "Epoch 6: error 0.003347513673361391 accuracy 99.17355371900827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m targets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     19\u001b[0m     ins, tgs \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     20\u001b[0m     ins \u001b[38;5;241m=\u001b[39m ins\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torchvision\\datasets\\folder.py:232\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 232\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torchvision\\datasets\\folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\torchvision\\datasets\\folder.py:251\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    250\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\PIL\\Image.py:915\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    919\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\INStallle\\envs\\torch_p39\\lib\\site-packages\\PIL\\ImageFile.py:255\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    252\u001b[0m         )\n\u001b[0;32m    254\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 255\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "\n",
    "total_acc = []\n",
    "total_loss = []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device \", device)\n",
    "\n",
    "network.train()\n",
    "network.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    loss_epoch = 0\n",
    "    for data in train_loader:\n",
    "        ins, tgs = data\n",
    "        ins = ins.to(device)\n",
    "        tgs = tgs.to(device)\n",
    "        # redimensionam tensor-ul input\n",
    "        #print(ins.shape)\n",
    "        #print()\n",
    "        #print(tgs.shape)\n",
    "        \n",
    "        # seteaza toti gradientii la zero, deoarece PyTorch acumuleaza valorile lor dupa mai multe backward passes\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # se face forward propagation -> se calculeaza predictia\n",
    "        output = network(ins)\n",
    "\n",
    "        # se calculeaza eroarea/loss-ul\n",
    "        loss = criterion(output, tgs)\n",
    "\n",
    "        # se face backpropagation -> se calculeaza gradientii\n",
    "        loss.backward()\n",
    "\n",
    "        # se actualizează weights-urile\n",
    "        opt.step()\n",
    "\n",
    "        loss_epoch = loss_epoch + loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            network.eval()\n",
    "            current_predict = network(ins)\n",
    "\n",
    "            # deoarece reteaua nu include un strat de softmax, predictia finala (cifra) trebuie calculata manual\n",
    "            current_predict = nn.Softmax(dim=1)(current_predict)\n",
    "            current_predict = current_predict.argmax(dim=1)\n",
    "\n",
    "            if 'cuda' in device.type:\n",
    "                current_predict = current_predict.cpu().numpy()\n",
    "                current_target = tgs.cpu().numpy()\n",
    "            else:\n",
    "                current_predict = current_predict.numpy()\n",
    "                current_target = tgs.numpy()\n",
    "\n",
    "            # print(current_predict.shape)\n",
    "            predictions = np.concatenate((predictions, current_predict), axis=0)\n",
    "            targets = np.concatenate((targets, current_target))\n",
    "    \n",
    "    total_loss.append(loss_epoch/train_bs)\n",
    "    \n",
    "    # print(predictions.shape)\n",
    "    # print(len(targets))\n",
    "    # Calculam acuratetea\n",
    "    acc = np.sum(predictions==targets)/len(predictions)\n",
    "    total_acc.append(acc)\n",
    "    print(f'Epoch {ep}: error {loss_epoch/train_bs} accuracy {acc*100}')\n",
    "\n",
    "    # salvam ponderile modelului dupa fiecare epoca\n",
    "    torch.save(network, 'my_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Testarea networkului"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 85.71428571428571\n"
     ]
    }
   ],
   "source": [
    "# incarcam ponderile modelul antrenat\n",
    "network = torch.load('my_model.pth')\n",
    "\n",
    "test_labels = test_ds.targets\n",
    "predictions = []\n",
    "\n",
    "network.eval()\n",
    "for data in test_loader:\n",
    "    ins, tgs = data\n",
    "    ins = ins.to(device)\n",
    "\n",
    "    current_predict = network(ins)\n",
    "    current_predict = nn.Softmax(dim=1)(current_predict)\n",
    "    current_predict = current_predict.argmax(dim=1)\n",
    "\n",
    "    if 'cuda' in device.type:\n",
    "        current_predict = current_predict.cpu().numpy()\n",
    "    else:\n",
    "        current_predict = current_predict.numpy()\n",
    "    predictions = np.concatenate((predictions, current_predict))\n",
    "\n",
    "acc = np.sum(predictions == test_labels)/len(predictions)\n",
    "print(f'Test accuracy is {acc*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercițiu\n",
    "\n",
    "Antrenați o rețea convoluțională pentru diagnosticarea COVID-19 în radiografii pulmonare și raportați performanțele pe setul de testare. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
